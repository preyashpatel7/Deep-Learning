{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting textual data into numerical vectors\n",
    "\n",
    "1) Frequency based methods: TF-IDF, CountVectorizer\n",
    "2) prediction-based embeddings: word2vec, gloVe -> captures semantic relationships, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "statistical measure that evaluates how relevent a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times word appear in a document (term frequency), and the inverse document frequency of the word acress a set of documents.\n",
    "\n",
    "There are several ways of calculating TF. \n",
    "IDF refers to how common or rare word is in the entire document set. The closer it is to 0, the more common a word is otherwise it will be closer to 1. Calculated by taking total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n",
    "\n",
    "Multiplying these two numbers result in the TF-IDF score of the word in a document. The higher the score, the more relevant that word is in that particular document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer \n",
    "\n",
    "it converts a collection of text documents into a matrix of toekn counts. This means text converted into vectors which contains number of times the word has appeared in the sentence. These word vectors are word embeddings(frequency based).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction based embeddings \n",
    "\n",
    "uses pre-trained models to create vector representations using neural networks. These types of models are suitable when we need to have large contextual information.\n",
    "\n",
    "word2vec: 300-D vector\n",
    "\n",
    "word2vec used for learning word embeddings using shallow neural networks.\n",
    "it contains vector representations of around 50 billion words.\n",
    "similar words have similar vectors.\n",
    "distance measured using cosine distance between two vectors.\n",
    "represents each word as a 300-D vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is not single algoritm, it is combination of two techniques:\n",
    "\n",
    "CBOW (Continuous Bag of Words)\n",
    "Skip-Gram model\n",
    "\n",
    "\n",
    "1. Bag of words: Treat each sentence as a seperate document, will make list of all unique words from all docs excluding punctuation. Now create vector for each senetnce where assign value 1 to those indexes for word in sentence and rest to 0. Vector length will be number of unique words in our dataset. Here we do not take into account context.\n",
    "\n",
    "2. CBOW: modefied version of BOW. we use context of each word as the input and then try to predict the next word corresponding to the context. We try to predict next word from given word, so in this process of predicting the target word, model (neural netwrok) will learn the vector representation of the target word, and it takes into account the context of the word which was not available in BOW.\n",
    "\n",
    "We can give C number of context vectors to the model, and the hidden layer will average the vectors and produce final vector for a perticular word based on the C context vectors. \n",
    "\n",
    "![CBOW](datasets/CBOW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Skip-gram Model: follows same strategy as the CBOW but in the output, model tries to predict the context words as the output given an input word.\n",
    "\n",
    "If we provide one word to model, then model will predict 1 context word to right and left of that given word. it's called 1-context CBOW model. We can predict more words too.\n",
    "\n",
    "![skip-gram](datasets/skip%20gram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage of CBOW model:\n",
    "probabilistic nature and able to perform superior to deterministic methods.\n",
    "does not need large RAM requirements.\n",
    "\n",
    "Disadvantages of CBOW model:\n",
    "does not capture two sementics for a single word, so we use skip-gram model for it. Meaning skip-gram model will have two different vector for single word for different contextual meaning. Ex: Apple as a company and a fruit.\n",
    "\n",
    "\n",
    "Word2vec uses both techniques in tandem to create more realistic word vector representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1570M  100 1570M    0     0  22.5M      0  0:01:09  0:01:09 --:--:-- 21.8M    0  0:01:11  0:01:02  0:00:09 29.6M8  0:00:01 27.5M\n"
     ]
    }
   ],
   "source": [
    "# New URL for the Google News vectors\n",
    "!curl -L -o ./datasets/GoogleNews-vectors-negative300.bin.gz \"https://github.com/mmihaltz/word2vec-GoogleNews-vectors/raw/master/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('./datasets/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6365211]]\n"
     ]
    }
   ],
   "source": [
    "v_banana = word_vectors['banana']\n",
    "v_mango = word_vectors['mango']\n",
    "\n",
    "print(cosine_similarity([v_banana], [v_mango]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tells mango and banana are 63% similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd_one_out(words,word_vectors):\n",
    "    all_word_vectors = [word_vectors[w] for w in words]\n",
    "    avg_vector = np.mean(all_word_vectors,axis=0)\n",
    "    odd_one_out = None\n",
    "    min_sim = 1.0\n",
    "\n",
    "    for w in words:\n",
    "        sim = cosine_similarity([word_vectors[w]],[avg_vector])\n",
    "        if sim < min_sim:\n",
    "            min_sim = sim\n",
    "            odd_one_out = w\n",
    "\n",
    "    return odd_one_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "party\n"
     ]
    }
   ],
   "source": [
    "list_of_words = [\"apple\",\"mango\",\"party\",\"juice\",\"orange\"]\n",
    "\n",
    "print(odd_one_out(list_of_words,word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "B - A = D - C\n",
    "D = B - A + C\n",
    "'''\n",
    "\n",
    "def word_analogies(A,B,C,word_vectors):\n",
    "    A,B,C = A.lower(),B.lower(),C.lower()\n",
    "    max_sim=-100\n",
    "    D= None\n",
    "\n",
    "    words = word_vectors.index_to_key\n",
    "    WA,WB,WC = word_vectors[A],word_vectors[B],word_vectors[C]\n",
    "\n",
    "    for w in words:\n",
    "        if w in [A,B,C]:\n",
    "            continue\n",
    "        w_vector = word_vectors[w]\n",
    "        sim = cosine_similarity([WB-WA],[w_vector-WC])\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            D = w\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = word_analogies(\"Man\",\"Woman\",\"King\",word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen\n"
     ]
    }
   ],
   "source": [
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071)]\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar(positive = ['woman','king'], negative = ['man'],topn=1)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
